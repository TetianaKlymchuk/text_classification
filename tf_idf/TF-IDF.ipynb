{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In information retrieval, tf–idf, TF-IDF, or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from statistics import mean\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_reviews.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranking:\n",
    "    \"\"\"\n",
    "    Class that handles ranking algorithms.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    directory: where will the dictionaries be loaded/saved.\n",
    "    document_frequencies_path: path to save document_frequencies dict.\n",
    "    term_frequencies_path: path to save term_frequencies dict.\n",
    "    document_length_path: path to save document_length dict.\n",
    "    document_frequencies: dict <word> -> <number of documents appearing>\n",
    "    term_frequencies: dict <document_id> -> <word> -> <number of occurrences>\n",
    "    document_length: dict <document_id> -> <number of words>\n",
    "    num_documents: number of documents stored.\n",
    "    ids: ids of the documents stored.\n",
    "    avg_length: average length of the documents stored.\n",
    "\n",
    "    Methods\n",
    "    --------\n",
    "    _get_dictionaries: Loads the dictionaries needed to calculate the scoring functions.\n",
    "    _get_dictionaries_from_file: Given a file_name returns the dictionary if the file exists or returns an empty dict.\n",
    "    add_document: It adds a document to the dictionaries needed to compute the documents scores.\n",
    "    delete_document: It deletes the given document from the dictionaries needed to compute the documents scores.\n",
    "    save: Saves the dictionaries to a given path.\n",
    "    get_tfidf_score: Given a query, computes the TFIDF score for every document in the corpus.\n",
    "    get_bm25_scores: Given a query, computes the BM25+ score for every document in the corpus.\n",
    "    most_similar_threshold: Given a query it returns the k most relevant documents or the ones\n",
    "                        that are over a given threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, directory='Data/ranking_dict/'):\n",
    "        \"\"\"\n",
    "        Init Ranking class.\n",
    "        :param directory: where will the dictionaries be loaded/saved.\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        self.directory = directory\n",
    "        self.document_frequencies_path = os.path.join(directory, 'document_frequencies.p')\n",
    "        self.term_frequencies_path = os.path.join(directory, 'term_frequencies.p')\n",
    "        self.document_length_path = os.path.join(directory, 'document_length.p')\n",
    "\n",
    "        self._get_dictionaries()\n",
    "        self.num_documents = len(self.document_frequencies)\n",
    "        self.ids = list(self.term_frequencies.keys())\n",
    "\n",
    "        if len(self.document_length) > 0:\n",
    "            self.avg_length = mean(self.document_length.values())\n",
    "        else:\n",
    "            self.avg_length = None\n",
    "\n",
    "    def _get_dictionaries(self):\n",
    "        \"\"\"\n",
    "        Loads the dictionaries needed to calculate the scoring functions.\n",
    "        \"\"\"\n",
    "\n",
    "        self.document_frequencies = self._get_dictionaries_from_file(self.document_frequencies_path)\n",
    "        self.term_frequencies = self._get_dictionaries_from_file(self.term_frequencies_path)\n",
    "        self.document_length = self._get_dictionaries_from_file(self.document_length_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_dictionaries_from_file(file_name):\n",
    "        \"\"\"\n",
    "        Given a file_name returns the dictionary if the file exists or returns an empty dict.\n",
    "        :param file_name: path where the file is to be found. <str>\n",
    "        :return dictionary stored in the pickle file if found, emtpy dict if not found. <dict>\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.isfile(file_name):\n",
    "            with open(file_name, 'rb') as fp:\n",
    "                return pickle.load(fp)\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    def add_document(self, document_id, document):\n",
    "        \"\"\"\n",
    "        It adds a document to the dictionaries needed to compute the documents scores.\n",
    "        It gets rid off punctuation.\n",
    "        :param document_id: unique id for the document. <str>\n",
    "        :param document: document to be added. It can be either a string or a json file from amazon api.\n",
    "        \"\"\"\n",
    "        if document_id in self.ids:\n",
    "            raise Exception(\"You provided an ID it's already stored.\")\n",
    "\n",
    "        words = split_words(document)\n",
    "\n",
    "        actual_frequencies = {}\n",
    "        words_set = set()\n",
    "        length = 0\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in string.punctuation:\n",
    "                continue\n",
    "            length += 1\n",
    "            if word in actual_frequencies:\n",
    "                actual_frequencies[word] += 1\n",
    "            else:\n",
    "                actual_frequencies[word] = 1\n",
    "            words_set.add(word)\n",
    "        for word in words_set:\n",
    "            if word in self.document_frequencies:\n",
    "                self.document_frequencies[word] += 1\n",
    "            else:\n",
    "                self.document_frequencies[word] = 1\n",
    "        self.document_length[document_id] = length\n",
    "        self.term_frequencies[document_id] = actual_frequencies\n",
    "\n",
    "        # RECALCULATE VALUES\n",
    "        self.avg_length = mean(self.document_length.values())\n",
    "        self.num_documents = len(self.document_frequencies)\n",
    "        self.ids = list(self.term_frequencies.keys())\n",
    "\n",
    "        self.save()\n",
    "\n",
    "    def delete_document(self, document_id):\n",
    "        \"\"\"\n",
    "        It deletes the given document from the dictionaries needed to compute the documents scores.\n",
    "        :param document_id: unique id for the document. <str>\n",
    "\n",
    "        :raises Exception if the given document_id is not stored.\n",
    "        \"\"\"\n",
    "\n",
    "        if document_id not in self.ids:\n",
    "            raise Exception(\"You provided an ID it's not stored.\")\n",
    "\n",
    "        word_set = self.term_frequencies[document_id].keys()\n",
    "        for word in word_set:\n",
    "            self.document_frequencies[word] -= 1\n",
    "            if self.document_frequencies[word] == 0:\n",
    "                del self.document_frequencies[word]\n",
    "\n",
    "        del self.document_length[document_id]\n",
    "        del self.term_frequencies[document_id]\n",
    "\n",
    "        # RECALCULATE VALUES\n",
    "        self.ids = list(self.term_frequencies.keys())\n",
    "        if len(self.ids) != 0:\n",
    "            self.avg_length = mean(self.document_length.values())\n",
    "            self.num_documents = len(self.document_frequencies)\n",
    "        else:\n",
    "            self.avg_length = 0\n",
    "            self.num_documents = 0\n",
    "\n",
    "        self.save()\n",
    "\n",
    "    def save(self, path=None):\n",
    "        \"\"\"\n",
    "        Saves the dictionaries to a given path. If path is None, it will save into the directory\n",
    "        specified in the init method.\n",
    "        :param path: directory where the dictionaries are going to be saved. <str> (default: None)\n",
    "        \"\"\"\n",
    "\n",
    "        if path is None:\n",
    "            with open(self.document_frequencies_path, 'wb') as fp:\n",
    "                pickle.dump(self.document_frequencies, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(self.term_frequencies_path, 'wb') as fp:\n",
    "                pickle.dump(self.term_frequencies, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(self.document_length_path, 'wb') as fp:\n",
    "                pickle.dump(self.document_length, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        else:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "            document_frequencies_path = os.path.join(path, 'document_frequencies.p')\n",
    "            term_frequencies_path = os.path.join(path, 'term_frequencies.p')\n",
    "            document_length_path = os.path.join(path, 'document_length.p')\n",
    "\n",
    "            with open(document_frequencies_path, 'wb') as fp:\n",
    "                pickle.dump(self.document_frequencies, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(term_frequencies_path, 'wb') as fp:\n",
    "                pickle.dump(self.term_frequencies, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open(document_length_path, 'wb') as fp:\n",
    "                pickle.dump(self.document_length, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def get_tfidf_scores(self, query, filtered_ids=None):\n",
    "        \"\"\"\n",
    "        Given a query, computes the TFIDF score for every document in the corpus.\n",
    "        :param query: query to find relevant documents from. <str>\n",
    "        :param filtered_ids: if specified, it will only compute the score of the provided ids.\n",
    "                        If None, it will compute the score for all the documents. <list of str (ids)>\n",
    "                        (default: None)\n",
    "        :return: dictionary with document_id as key and the score as value. <dict>. <document_id> -> <score>.\n",
    "        \"\"\"\n",
    "        length = {}\n",
    "        scores = {}\n",
    "\n",
    "        if filtered_ids is None:\n",
    "            ids = self.ids\n",
    "        else:\n",
    "            ids = filtered_ids\n",
    "\n",
    "        for ind in ids:\n",
    "            length[ind] = 0\n",
    "            scores[ind] = 0\n",
    "\n",
    "        for term in re.findall(r\"[\\w']+|[.,!?;]\", query.strip()):\n",
    "            term = term.lower()\n",
    "            if (term not in self.document_frequencies) or (len(term) <= 2):\n",
    "                continue\n",
    "            df = self.document_frequencies[term]\n",
    "            wq = np.log(self.num_documents / df)\n",
    "            for ind in ids:\n",
    "                if ind not in self.ids:\n",
    "                    continue\n",
    "                document_dict = self.term_frequencies[ind]\n",
    "                if term not in document_dict:\n",
    "                    scores[ind] += 0\n",
    "                    continue\n",
    "                tf = document_dict[term]\n",
    "                length[ind] += tf ** 2\n",
    "                wd = 1 + np.log(tf)\n",
    "                scores[ind] += wq * wd\n",
    "        for ind in ids:\n",
    "            if length[ind] == 0:\n",
    "                continue\n",
    "            scores[ind] /= np.sqrt(length[ind])\n",
    "\n",
    "        return scores\n",
    "    def most_similar(self, query, threshold=None, k=100, func='tfidf', filtered_ids=None):\n",
    "        \"\"\"\n",
    "        Given a query it returns the k most relevant documents or the ones that are over a given threshold.\n",
    "        :param query: query to find relevant documents from. <str>\n",
    "        :param threshold: min scoring value of the sorted document_ids. <int> (default: None)\n",
    "        :param k: number of most relevant documents to return. <int> (default: 100)\n",
    "        :param func: whether you want to use bm25+ or tfidf scoring. <str> (default: bm25)\n",
    "        :param filtered_ids: if specified, it will only compute the score of the provided ids.\n",
    "                        If None, it will compute the score for all the documents. <list of str (ids)>\n",
    "                        (default: None)\n",
    "        :return: list of strings\n",
    "        \"\"\"\n",
    "        if func == 'tfidf':\n",
    "            scores = self.get_tfidf_scores(query, filtered_ids)\n",
    "        most_similar = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if threshold is None:\n",
    "            ids = [doc_id for doc_id, _ in most_similar[:k]]\n",
    "\n",
    "        else:\n",
    "            ids = [doc_id for doc_id, score in most_similar if score >= threshold]\n",
    "            if k is not None:\n",
    "                ids = ids[:k]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def get_most_similar(self, query, k=1):\n",
    "\n",
    "        tfidf_scores = self.get_tfidf_scores(query)\n",
    "\n",
    "        candidates_scores = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        # best_index, long_score = candidate_scores[0]\n",
    "\n",
    "        # return best_index, long_score\n",
    "        return candidates_scores[:k]\n",
    "\n",
    "    def get_candidates(self, df, column):\n",
    "\n",
    "        self.reset()\n",
    "        for index, text in zip(df.index, df[column]):\n",
    "            if index not in self.ids:\n",
    "                self.add_document(index, text)\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        for document_id in self.ids:\n",
    "            self.delete_document(document_id)\n",
    "\n",
    "        self.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"Data/ranking_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from langdetect import detect\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "\n",
    "def save_dictionaries(df):\n",
    "    \"\"\"\n",
    "    Calculates the dictionaries needed to compute TFIDF score.\n",
    "        term_frequencies: list of dicts of term frequencies within a document.\n",
    "        document_frequencies: number of documents containing a given term.\n",
    "        document_length\n",
    "    The dictionaries will be stored in the directory Data/ranking_dict/\n",
    "    \"\"\"\n",
    "    directory = 'Data/ranking_dict/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    # CREATE TEXT DICTIONARIES\n",
    "\n",
    "    term_frequencies = {}  # dict of dicts id -> word -> frequency within a document\n",
    "    document_frequencies = {}  # dict word -> number of documents containing the term\n",
    "    document_length = {}  # dict id -> document length\n",
    "\n",
    "    for id in list(df.id):\n",
    "        term_frequencies[id] = {}\n",
    "        document_length[id] = 0\n",
    "\n",
    "    print('Processing the corpus...\\n')\n",
    "    for id, document in tqdm(zip(list(df.id), list(df.review))):\n",
    "        actual_frequencies = {}\n",
    "        words_set = set()\n",
    "        length = 0\n",
    "        if type(document) != type('a'):\n",
    "            continue\n",
    "        for word in re.findall(r\"[\\w']+|[.,!?;]\", document.strip()):\n",
    "            word = word.lower()\n",
    "            if word in string.punctuation:\n",
    "                continue\n",
    "            length += 1\n",
    "            if word in actual_frequencies:\n",
    "                actual_frequencies[word] += 1\n",
    "            else:\n",
    "                actual_frequencies[word] = 1\n",
    "            words_set.add(word)\n",
    "        for word in words_set:\n",
    "            if word in document_frequencies:\n",
    "                document_frequencies[word] += 1\n",
    "            else:\n",
    "                document_frequencies[word] = 1\n",
    "        document_length[id] = length\n",
    "        term_frequencies[id] = actual_frequencies\n",
    "\n",
    "    # Save dictionaries into files\n",
    "    with open('Data/ranking_dict/document_frequencies.p', 'wb') as fp:\n",
    "        pickle.dump(document_frequencies, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('Data/ranking_dict/term_frequencies.p', 'wb') as fp:\n",
    "        pickle.dump(term_frequencies, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('Data/ranking_dict/document_length.p', 'wb') as fp:\n",
    "        pickle.dump(document_length, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(len(term_frequencies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the corpus...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000it [00:05, 4219.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "save_dictionaries(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ranking(\"Data/ranking_dict/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data/ranking_dict/document_frequencies.p'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5814_8',\n",
       " '2381_9',\n",
       " '7759_3',\n",
       " '3630_4',\n",
       " '9495_8',\n",
       " '8196_8',\n",
       " '7166_2',\n",
       " '10633_1',\n",
       " '319_1',\n",
       " '8713_10']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2190_2', 7.641138076110774)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_most_similar(\"film about car crash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['**Possible Spoilers Ahead**<br /><br />     Jason (a.k.a. Herb) Evers is a brilliant brain surgeon who, along with wife Virginia Leith, is involved in the most lackluster onscreen car crash ever. Leith is decapitated and the doctor takes her severed noggin back to his mansion and rejuvenates the head in his lab. The mansion\\'s exterior was allegedly filmed at Tarrytown\\'s Lyndhurst estate; the lab scenes were apparently shot in somebody\\'s basement. The bandaged head is kept alive on \\\\lab equipment\\\\\" that\\'s almost cheap-looking enough for Ed Wood. Some of the library music\\x96the movie\\'s high point\\x96later turned up in Andy Milligan\\'s THE BODY BENEATH. Leith\\'s head has some heavy metaphysical discourses with another of Ever\\'s misfires, a mutant chained in the closet. Meanwhile, the good doc prowls strip joints looking for a body worthy of his wife\\'s gabby noodle. The ending, in uncut prints, features some ahead-of-its-time splatter and dismemberment when the zucchini-headed monster comes out of the closet to bring the movie to a welcome close. This thing took three years to be released and then, audiences gave it the bad reception it richly deserved. Between this, PLAN 9 FROM OUTER SPACE and a few others, 1959 should have been declared The Year Of The Turkey.\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.id=='2190_2'].review.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = model.most_similar(\"emotional, romantic movie that involves brain work\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['**Possible Spoilers Ahead**<br /><br />     Jason (a.k.a. Herb) Evers is a brilliant brain surgeon who, along with wife Virginia Leith, is involved in the most lackluster onscreen car crash ever. Leith is decapitated and the doctor takes her severed noggin back to his mansion and rejuvenates the head in his lab. The mansion\\'s exterior was allegedly filmed at Tarrytown\\'s Lyndhurst estate; the lab scenes were apparently shot in somebody\\'s basement. The bandaged head is kept alive on \\\\lab equipment\\\\\" that\\'s almost cheap-looking enough for Ed Wood. Some of the library music\\x96the movie\\'s high point\\x96later turned up in Andy Milligan\\'s THE BODY BENEATH. Leith\\'s head has some heavy metaphysical discourses with another of Ever\\'s misfires, a mutant chained in the closet. Meanwhile, the good doc prowls strip joints looking for a body worthy of his wife\\'s gabby noodle. The ending, in uncut prints, features some ahead-of-its-time splatter and dismemberment when the zucchini-headed monster comes out of the closet to bring the movie to a welcome close. This thing took three years to be released and then, audiences gave it the bad reception it richly deserved. Between this, PLAN 9 FROM OUTER SPACE and a few others, 1959 should have been declared The Year Of The Turkey.\"']\n",
      "['**Possible Spoilers Ahead**<br /><br />     Jason (a.k.a. Herb) Evers is a brilliant brain surgeon who, along with wife Virginia Leith, is involved in the most lackluster onscreen car crash ever. Leith is decapitated and the doctor takes her severed noggin back to his mansion and rejuvenates the head in his lab. The mansion\\'s exterior was allegedly filmed at Tarrytown\\'s Lyndhurst estate; the lab scenes were apparently shot in somebody\\'s basement. The bandaged head is kept alive on \\\\lab equipment\\\\\" that\\'s almost cheap-looking enough for Ed Wood. Some of the library music\\x96the movie\\'s high point\\x96later turned up in Andy Milligan\\'s THE BODY BENEATH. Leith\\'s head has some heavy metaphysical discourses with another of Ever\\'s misfires, a mutant chained in the closet. Meanwhile, the good doc prowls strip joints looking for a body worthy of his wife\\'s gabby noodle. The ending, in uncut prints, features some ahead-of-its-time splatter and dismemberment when the zucchini-headed monster comes out of the closet to bring the movie to a welcome close. This thing took three years to be released and then, audiences gave it the bad reception it richly deserved. Between this, PLAN 9 FROM OUTER SPACE and a few others, 1959 should have been declared The Year Of The Turkey.\"']\n",
      "['**Possible Spoilers Ahead**<br /><br />     Jason (a.k.a. Herb) Evers is a brilliant brain surgeon who, along with wife Virginia Leith, is involved in the most lackluster onscreen car crash ever. Leith is decapitated and the doctor takes her severed noggin back to his mansion and rejuvenates the head in his lab. The mansion\\'s exterior was allegedly filmed at Tarrytown\\'s Lyndhurst estate; the lab scenes were apparently shot in somebody\\'s basement. The bandaged head is kept alive on \\\\lab equipment\\\\\" that\\'s almost cheap-looking enough for Ed Wood. Some of the library music\\x96the movie\\'s high point\\x96later turned up in Andy Milligan\\'s THE BODY BENEATH. Leith\\'s head has some heavy metaphysical discourses with another of Ever\\'s misfires, a mutant chained in the closet. Meanwhile, the good doc prowls strip joints looking for a body worthy of his wife\\'s gabby noodle. The ending, in uncut prints, features some ahead-of-its-time splatter and dismemberment when the zucchini-headed monster comes out of the closet to bring the movie to a welcome close. This thing took three years to be released and then, audiences gave it the bad reception it richly deserved. Between this, PLAN 9 FROM OUTER SPACE and a few others, 1959 should have been declared The Year Of The Turkey.\"']\n"
     ]
    }
   ],
   "source": [
    "for id in ranking:\n",
    "    print(df[df.id=='2190_2'].review.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uic",
   "language": "python",
   "name": "uic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
